# ğŸŒ€ NLP with Disaster Tweets

![Top Words](outputs/Twitt.png)

## ğŸ“Œ Project Overview

This project focuses on classifying tweets to determine whether they refer to real-world disasters. The model enables improved emergency response, real-time crisis monitoring, and insights from social media data. It leverages a combination of classical machine learning and deep learning techniques, including hybrid models that incorporate both text and structured data inputs.

---

## ğŸ—‚ï¸ Table of Contents

- [Project Structure](#project-structure)
- [Dataset Overview](#dataset-overview)
- [Exploratory Data Analysis (EDA)](#exploratory-data-analysis-eda)
- [Data Preprocessing](#data-preprocessing)
- [Modeling Approaches](#modeling-approaches)
- [Model Evaluation](#model-evaluation)
- [Visualizations](#visualizations)
- [Key Findings](#key-findings)
- [Recommendations](#recommendations)
- [Installation](#installation)
- [Usage](#usage)
- [Contributors](#contributors)
- [License](#license)

---

## ğŸ“ Project Structure

```plaintext
project-root/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv
â”‚   â””â”€â”€ test.csv
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Project_NLP_with_Disaster_Tweets_v_2.ipynb
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ BiLSTM-Based Hybrid ModelSchema.drawio.png
â”‚   â”œâ”€â”€ Model Comparison Radar Chart.png
â”‚   â””â”€â”€ Twitt.png
â”‚
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ Project_report_Disaster_tweet.pdf
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸ“Š Dataset Overview

- **Source**: Kaggle ([NLP with Disaster Tweets](https://www.kaggle.com/competitions/nlp-getting-started))
- **Format**: `train.csv`
  - `text`: Tweet content
  - `keyword`: Optional disaster-related keyword
  - `location`: Optional user-defined location
  - `target`: Binary label (1: disaster, 0: non-disaster)

---

## ğŸ” Exploratory Data Analysis (EDA)

- **Missing Values**:
  - `text` and `target`: None
  - `keyword`: ~60 missing (filled/dropped)
  - `location`: ~33% missing (excluded in text-only models)
- **Distribution**:
  - Imbalanced classes: ~57% non-disaster, 43% disaster
- **Tweet Length**: Mostly <140 characters
- **Keyword Insight**: High correlation of keywords like *fire*, *earthquake*, *accident* with disaster tweets

---

## ğŸ§¹ Data Preprocessing

- Lowercased text
- Removed punctuation, URLs, mentions, hashtags, stopwords
- Tokenized, lemmatized, and stemmed
- Encoded with:
  - TF-IDF (ML models)
  - Tokenizer + padding (Deep models)
  - BERT tokenizer (Transformer models)

---

## ğŸ§  Modeling Approaches

### 1. Text-Only Pipeline
- **Input**: `text` column  
- **Used for**: TF-IDF + ML models and BERT  
- **Encoding**: TF-IDF or BERT tokenizer  
- **Pros**: Simplicity, low preprocessing cost  

### 2. Hybrid Pipeline
- **Inputs**: `text`, `keyword`, `location`, text metadata  
- **Used for**: CNN, BiLSTM with auxiliary structured features  
- **Architecture**:  
  - Branch 1: Embedding â†’ Conv1D/BiLSTM â†’ Pooling  
  - Branch 2: Structured features â†’ Dense  
  - Combined: Concatenate â†’ Dense â†’ Output  

![Model Architecture](outputs/BiLSTM-Based%20Hybrid%20ModelSchema.drawio.png)

---

## ğŸ“ˆ Model Evaluation

All models were evaluated using:
- **Accuracy**
- **Precision**
- **Recall**
- **F1-Score**

---

## ğŸ“Š Visualizations

### Model Comparison Radar Chart
![Model Comparison Radar Chart](outputs/Model%20Comparison%20Radar%20Chart.png)

---

## ğŸ”‘ Key Findings

- **Top classical model**: Logistic Regression
- **Best recall**: BERT (0.77), BiLSTM (0.74)
- **Best precision**: SVM (0.86), CNN (0.88)
- **Most balanced**: BiLSTM, Logistic Regression
- **Best lightweight**: Naive Bayes
- **Worst performers**: KNN, PCA + Logistic Regression

---

## âœ… Recommendations

| Business Scenario                            | Recommended Model         | Reason                                                                 |
|---------------------------------------------|----------------------------|------------------------------------------------------------------------|
| Minimize missed disasters (High Recall)     | BERT, BiLSTM               | Detects most true disaster tweets                                      |
| Minimize false positives (High Precision)   | SVM, Dual Input CNN        | Fewest incorrect disaster flags                                        |
| Balanced classification (F1-Score)          | BiLSTM, Logistic Regression| Balanced performance, interpretable                                    |
| Fast & simple deployment                    | Logistic Regression, NB    | Lightweight, accurate, easy to maintain                               |
| Scalability with high-volume data           | XGBoost                    | Scalable, high performance                                             |

---

## âš™ï¸ Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/Kovalivska/disaster-tweet-classification.git
   cd disaster-tweet-classification
   
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸš€ Usage

1. **Launch Jupyter Notebook**:
   ```bash
   jupyter notebook
   ```

2. **Open and run**:
   Navigate to `notebooks/Project_NLP_with_Disaster_Tweets_v_2.ipynb` and execute all cells sequentially.

3. **Outputs**:
   - All results (visuals, diagrams) are saved in the `outputs/` folder.
   - Final project summary is provided in the `reports/` folder.

---

## ğŸ‘¥ Contributors

 Svitlana Kovalivska â€“ Data Scientist & Developer  


---

## ğŸ“œ License

This project is licensed under the MIT License. See the [LICENSE](./LICENSE) file for details.
